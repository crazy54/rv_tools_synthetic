# AI Configuration for RVTools Synthetic Data Generator

This script supports AI-assisted data generation to enhance the realism and context-awareness of the synthetic VMware environment data. This feature is optional and requires some setup depending on the chosen AI provider.

## General AI Usage (Command Line)

To enable AI-assisted data generation, use the following CLI arguments:

*   `--use_ai`: This flag enables the AI features. Without it, the script will rely purely on its internal random and rule-based generation.
*   `--ai_provider <provider_name>`: Specifies which AI backend to use.
    *   Choices: `mock` (default), `openai`, `ollama`.
    *   Example: `--use_ai --ai_provider openai`

Currently, real AI calls (OpenAI/Ollama) are implemented for the following CSV types: `vInfo`, `vHost`, `vDisk`, `vNetwork`, `vCluster`, and `vDatastore`. For other CSV types, if `--use_ai` is enabled, their specific mocked AI data will be used.

## OpenAI Setup

1.  **API Key**: You need an API key from OpenAI. Visit [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys) to create one.
2.  **Set Environment Variable**: The script expects your OpenAI API key to be set as an environment variable named `OPENAI_API_KEY`.
    *   On Linux/macOS: `export OPENAI_API_KEY='your_api_key_here'`
    *   On Windows (PowerShell): `$env:OPENAI_API_KEY='your_api_key_here'`
    *   You may want to add this to your shell's profile file (e.g., `.bashrc`, `.zshrc`, PowerShell Profile) for persistence.
3.  **Model Used**: The script currently defaults to using `gpt-4o-mini` for OpenAI calls, which offers a good balance of capability and cost. This might be configurable in future versions.
4.  **Costs**: Be aware that using the OpenAI API will incur costs based on your usage.

## Ollama (Local LLM) Setup

Ollama allows you to run powerful open-source language models locally on your own hardware.

1.  **Install Ollama**: Download and install Ollama from [https://ollama.com/](https://ollama.com/).
2.  **Pull a Model**: You need to download a model for Ollama to serve. Examples:
    *   `ollama pull llama3` (Meta's Llama 3, good general model)
    *   `ollama pull mistral` (Mistral AI's model)
    *   `ollama pull phi3` (Microsoft's Phi-3)
    Refer to the Ollama library for available models.
3.  **Ensure Ollama Server is Running**: The Ollama application (Desktop) or server (`ollama serve` via CLI) must be running in the background for the script to connect to it. It typically runs on `http://localhost:11434`.
4.  **Specify Model in CLI**: Use the `--ollama_model_name <model_you_pulled>` argument when running the script.
    *   Example: `--use_ai --ai_provider ollama --ollama_model_name llama3`
    *   If the specified model is not available in your Ollama instance, the call will likely fail, and the script will fall back to mock data.

## LangChain Framework

This script uses the [LangChain](https://www.langchain.com/) framework to interact with the different AI providers (OpenAI, Ollama). This allows for a more flexible and maintainable AI integration. You'll need to have the necessary LangChain packages installed in your Python environment if you intend to use the real AI features:
*   `langchain`
*   `langchain-openai` (for OpenAI)
*   `langchain-community` (which includes Ollama integrations)

You can typically install these using pip:
`pip install langchain langchain-openai langchain-community`

## Troubleshooting & Notes

*   **Fallback to Mock**: If a real AI call is attempted (e.g., `--use_ai --ai_provider openai`) but fails (due to missing API key, network issue, model not available for Ollama, library not installed), the script will print a warning and automatically fall back to using the internal mock AI data for that specific CSV/entity.
*   **Resource Usage**: Running LLMs locally with Ollama can be resource-intensive, especially for larger models. Ensure your hardware (CPU, RAM, GPU if applicable) is sufficient.
*   **Output Quality**: The quality and format of data generated by real AI calls can vary. The prompts have been engineered to request JSON, but occasionally models might deviate. The script includes basic validation for some fields.
